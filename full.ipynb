{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[43membedded_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "extracted_data = load_pdf(\"data/\")\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key='pcsk_2kFykp_KhvdnsyhiF5oM2RS9iJqiyFvQNznDBkqw12TocYxz7VzdT82aiWqAt9rud11M7H')\n",
    "index = pc.create_index(\n",
    "    name='quickstart',\n",
    "    dimension=384,\n",
    "    metric='cosine',\n",
    "    deletion_protection='enabled',\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws',\n",
    "        region='us-east-1'\n",
    "    )\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "print(f\"Embedding Dimensions: {embedding_dim}\")\n",
    "from langchain_pinecone import Pinecone\n",
    "\n",
    "vectorstore = Pinecone(index=index, embedding=embedding_model)\n",
    "\n",
    "import math\n",
    "\n",
    "# Function to split vectors into smaller batches\n",
    "def batch_vectors(vectors, batch_size):\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        yield vectors[i : i + batch_size]\n",
    "\n",
    "# Example: Split data into batches of size 100\n",
    "batch_size = 100  # Adjust this size based on your payload\n",
    "for batch in batch_vectors(vectors, batch_size):\n",
    "    upsert_response = index.upsert(\n",
    "        vectors=batch,\n",
    "        namespace=\"example-namespace\"\n",
    "    )\n",
    "print(\"All vectors upserted successfully!\")\n",
    "\n",
    "# Ensure each chunk is a string and create Document objects\n",
    "documents = [Document(page_content=str(chunk)) for chunk in text_chunks]\n",
    "\n",
    "# Extract the text content from the documents (list of strings)\n",
    "text_content = [doc.page_content for doc in documents]\n",
    "\n",
    "# Embed the documents using the Hugging Face model\n",
    "embedded_vectors = embedding_model.embed_documents(text_content)\n",
    "\n",
    "# Function to split data into smaller batches\n",
    "def batch_upsert(vectors, batch_size=100):\n",
    "    # Split the vectors into smaller batches\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        yield vectors[i:i + batch_size]\n",
    "\n",
    "# Prepare the upsert data in the required format\n",
    "pinecone_data = [\n",
    "    (\n",
    "        str(i),  # Unique ID for each vector (can be an incremental ID or anything unique)\n",
    "        embedded_vectors[i],  # The actual vector for the chunk\n",
    "        {\"text\": text_content[i]}  # Metadata: the original chunk as the 'text' field\n",
    "    )\n",
    "    for i in range(len(text_chunks))\n",
    "]\n",
    "\n",
    "# Split the upsert data into batches\n",
    "batches = batch_upsert(pinecone_data, batch_size=100)  # Adjust the batch_size as needed\n",
    "\n",
    "# Upsert the vectors in batches\n",
    "for batch in batches:\n",
    "    upsert_response = index.upsert(\n",
    "        vectors=batch,\n",
    "        namespace=\"example-namespace\"  # Replace with your desired namespace\n",
    "    )\n",
    "\n",
    "print(\"Upsert completed successfully.\")\n",
    "query_vector = embedding_model.embed_query(\"How are your allergies\") # Replace with your own query vector\n",
    "\n",
    "# Query the Pinecone index to find the top 5 most similar vectors\n",
    "query_response = index.query(\n",
    "    vector=query_vector,  # The query vector\n",
    "    top_k=5,              # Number of similar vectors to retrieve\n",
    "    include_metadata=True,  # Include metadata in the response (optional)\n",
    "    namespace=\"example-namespace\"  # Namespace you used during upsert\n",
    ")\n",
    "\n",
    "# Display the query results\n",
    "\"\"\"for match in query_response['matches']:\n",
    "    print(f\"{match['metadata']}\")\"\"\"\n",
    "print(query_response)\n",
    "prompt_template=\"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\"\"\"\n",
    "\n",
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}\n",
    "llm=CTransformers(model=\"D:\\pinecone\\model\\llama-2-13b-chat.ggmlv3.q5_1.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)\n",
    "user_input=input(f\"Input Prompt:\")    \n",
    "result=qa({\"query\": user_input}) print(\"Response : \", result[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
