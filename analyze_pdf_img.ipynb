{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy1jHGBRbQYO",
        "outputId": "b014c7d8-0880-4550-91b8-8b22e0c9e661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymupdf, pdf2image\n",
            "Successfully installed pdf2image-1.17.0 pymupdf-1.25.2\n"
          ]
        }
      ],
      "source": [
        "! pip install pymupdf    pdf2image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nSOpY7Zbc6ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import pathlib\n",
        "import textwrap\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyB9YV9z1hKhiYlqnUerhpuXMG\")\n",
        "\n",
        "\n",
        "def get_gemini_response(input,image,prompt):\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
        "    response = model.generate_content([input,image[0],prompt])\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oyQx6ddVdP-u"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_prompt = \"\"\"\n",
        "               You are an expert in understanding scan reports.\n",
        "               You will receive input images as scan and medical reports &\n",
        "               you will have to answer questions based on the input image\n",
        "               \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mki9f7JXfj4m"
      },
      "outputs": [],
      "source": [
        "uploaded_file = \"/content/NORMAL-NT-REPORT-1st-Part-1-889x1024.webp\"\n",
        "with open(uploaded_file, \"rb\") as f:\n",
        "    # Read the entire content of the file into bytes\n",
        "    # instead of using getvalue()\n",
        "    bytes_data = f.read()\n",
        "\n",
        "    image_parts = [\n",
        "        {\n",
        "            \"mime_type\": \"image/webp\",  # Specify the mime type directly\n",
        "            \"data\": bytes_data\n",
        "        }\n",
        "    ]\n",
        "    image_data = image_parts\n",
        "# Use a different variable name for the prompt, like 'user_prompt'\n",
        "user_prompt = \"Tell me about the scan\"  # Replace with the actual prompt\n",
        "response = get_gemini_response(input_prompt, image_data, user_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "c62jxEYwg29B",
        "outputId": "e1f25b1d-f2e9-459e-a6c9-cbce0f22875d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Here's a summary of the scan report:\\n\\n**Patient Information:**\\n\\n*   Age/Sex: 31 Years / Female\\n*   LMP Date: 02/03/2023\\n*   LMP EDD: 07/12/2023 (12 Weeks 1 Day)\\n*   Visit Number: 1\\n\\n**Indications:**\\n\\n*   First trimester screening\\n\\n**Procedure:**\\n\\n*   Real-time B-mode ultrasonography of the gravid uterus was performed.\\n*   Route: Transabdominal and Transvaginal\\n\\n**Findings:**\\n\\n*   Single intrauterine gestation.\\n\\n**Medical Notes:**\\n\\n*   Blood group: A1B+ve\\n*   Height: 159 cm\\n*   Weight: 48.2 kg\\n*   Marital History: 4 years, Non-consanguineous marriage\\n*   Menstrual History: Regular\\n*   Gravida: 2, Para: 1, Live: 1, Abortion: 0\\n*   Significant previous obstetric details: Nil\\n*   Medical/Surgical History: Lscs\\n\\n**Maternal:**\\n\\n*   Cervix measured 3.10 cm in length.\\n*   Right Uterine: 1.8\\n*   Left Uterine: 1.4\\n*   Mean PI: 1.6\\n\\n**Fetus:**\\n\\n*   Placenta: Anterior\\n*   Liquor: Normal\\n*   Fetal activity: Present\\n*   Cardiac activity: Present\\n*   Fetal heart rate: 154 bpm\\n\\n**Biometry:**\\n\\n*   CRL: 59 mm (12 Weeks 3 Days)\\n*   BPD: 21 mm (13 Weeks 2 Days)\\n*   HC: 75.43 mm (13 Weeks 1 Day)\\n*   AC: 58.14 mm (12 Weeks 4 Days)\\n\\n**Summary:** The scan report indicates a normal first-trimester screening with a single intrauterine gestation. Fetal and cardiac activity are present, and the fetal heart rate is within the normal range. The biometric measurements are consistent with the gestational age.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "clean_text = re.sub(r'\\n+', '\\n', response).strip()\n",
        "\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1oNLCz5op1g",
        "outputId": "c941c48c-4f16-4297-ccb5-9a96a1cd1629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "EI5UNroMpFC3",
        "outputId": "2fc1d44f-9f4d-4f52-9194-5bc726e20353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask a question about the PDF (or type 'exit' to quit): tell me about the benchmark\n",
            "\n",
            "ðŸ”¹ Text-Based Answer: The benchmark in the DeepSeek-R1 paper is a comprehensive suite of tests designed to evaluate the reasoning capabilities and general performance of Large Language Models (LLMs).  It covers a wide range of tasks, including:\n",
            "\n",
            "**Reasoning-focused:**\n",
            "\n",
            "* **Math:** AIME 2024, MATH-500, CNMO 2024 (Chinese National High School Mathematics Olympiad). These test the model's ability to solve complex mathematical problems, often requiring multi-step reasoning and logical deduction.\n",
            "* **Coding:** Codeforces, LiveCodeBench, SWE-Bench Verified, Aider.  These assess coding proficiency in various aspects, from competitive programming (Codeforces) to practical software engineering tasks (SWE-Bench, Aider) and code generation (LiveCodeBench).\n",
            "\n",
            "**Knowledge and General Capabilities:**\n",
            "\n",
            "* **General Knowledge:** MMLU, MMLU-Redux, MMLU-Pro, GPQA Diamond, SimpleQA, C-SimpleQA. These evaluate the model's understanding across various domains of knowledge, including humanities, social sciences, STEM, and more.  SimpleQA and C-SimpleQA focus on factual question answering.\n",
            "* **Language Understanding and Reasoning:** C-Eval (Chinese equivalent of MMLU), CLUEWSC (Chinese Winograd Schema Challenge), IFEval (Instruction Following Evaluation), FRAMES (Fact, Retrieval, And Multimodal Evaluation Set), DROP (a reading comprehension benchmark).  These test the model's ability to understand complex language, follow instructions, and perform reasoning based on text.\n",
            "\n",
            "**Open-ended Generation:**\n",
            "\n",
            "* **AlpacaEval 2.0:**  Assesses the model's performance on various instruction-following tasks, emphasizing length-controlled generation to mitigate biases.\n",
            "* **Arena-Hard:** A benchmark focusing on challenging open-domain question answering and creative writing scenarios.\n",
            "\n",
            "**Key characteristics of the benchmark:**\n",
            "\n",
            "* **Diversity:** The benchmark covers a broad spectrum of tasks, reflecting the multifaceted nature of intelligence and reasoning.\n",
            "* **Focus on Reasoning:**  A significant portion of the benchmark is dedicated to evaluating reasoning abilities, aligning with the paper's primary goal.\n",
            "* **Multilingual Evaluation:**  Includes both English and Chinese benchmarks, demonstrating the model's capabilities across different languages.\n",
            "* **Use of LLM Judges:** For open-ended generation tasks, the benchmark employs LLMs (specifically GPT-4-Turbo-1106) as judges for pairwise comparisons, reflecting a trend towards more nuanced evaluation methods.\n",
            "* **Emphasis on Pass@k:** The evaluation primarily uses the Pass@k metric, particularly Pass@1, which measures the probability of getting the correct answer within k generated samples.  This accounts for the variability inherent in generative models.\n",
            "* **Consensus@k (cons@k):**  For tasks like AIME 2024, consensus scoring (majority voting) is also used to assess performance.\n",
            "\n",
            "This diverse and rigorous benchmark provides a comprehensive evaluation of the reasoning and general capabilities of DeepSeek-R1 and its distilled versions, allowing for comparison with other leading LLMs.\n",
            "\n",
            "\n",
            "ðŸ”¹ Image Analysis (VLM): Here's a summary of the content in the images:\n",
            "\n",
            "**Image 1: Accuracy During Training**\n",
            "\n",
            "*   **Title:** \"DeepSeek-R1-Zero AIME accuracy during training\"\n",
            "*   **Content:** The image shows the accuracy of two different models (\"r1-zero-pass@1\" and \"r1-zero-cons@16\") during training. The x-axis represents the training steps, and the y-axis represents the accuracy.\n",
            "*   **Key Observations:**\n",
            "    *   The \"r1-zero-cons@16\" model (red line) consistently achieves higher accuracy than the \"r1-zero-pass@1\" model (blue line) throughout the training process.\n",
            "    *   Both models show an increasing trend in accuracy as the training progresses.\n",
            "    *   Two horizontal lines: A green dashed line indicating \"o1-0912-pass@1\" accuracy (around 0.74) and a purple dashed line indicating \"o1-0912-cons@64\" accuracy (around 0.83). These likely represent benchmark performances.\n",
            "\n",
            "**Image 2: Average Length per Response During Training**\n",
            "\n",
            "*   **Title:** \"DeepSeek-R1-Zero average length per response during training\"\n",
            "*   **Content:**  The image illustrates the average length of the response (likely in tokens or characters) generated by a model as training progresses. The x-axis shows training steps, and the y-axis represents the average length per response.\n",
            "*   **Key Observations:**\n",
            "    *   The average response length tends to increase as the training steps increase.  The line shows an overall upward trajectory.\n",
            "    *   There is a significant amount of variability (indicated by the shaded area) in the average response length at any given training step, implying that the model's output length fluctuates.\n",
            "\n",
            "Ask a question about the PDF (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import fitz  # PyMuPDF for text extraction\n",
        "from pdf2image import convert_from_path  # Convert PDF pages to images\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# ðŸ”¹ Configure Gemini API (Replace with your API Key)\n",
        "genai.configure(api_key=\"AIzaSyB9YV9z1hKhiYlqnUeMG1dZahGEYI\")\n",
        "\n",
        "def extract_pdf_text(pdf_path):\n",
        "    \"\"\"Extract text from a PDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)  # Open PDF\n",
        "    text_data = [page.get_text(\"text\") for page in doc]  # Extract text\n",
        "    return \"\\n\".join(text_data)  # Combine all text\n",
        "\n",
        "def extract_pdf_images(pdf_path):\n",
        "    \"\"\"Extract images from a PDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)  # Open PDF\n",
        "    images = []\n",
        "\n",
        "    for page in doc:\n",
        "        for img in page.get_images(full=True):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            img_bytes = base_image[\"image\"]\n",
        "            image = Image.open(io.BytesIO(img_bytes))\n",
        "            images.append(image)\n",
        "\n",
        "    return images  # List of images\n",
        "\n",
        "def ask_gemini_llm(pdf_text, question):\n",
        "    \"\"\"Ask a question to Gemini Pro (Text-only LLM).\"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-pro\")  # Text LLM\n",
        "    response = model.generate_content([pdf_text, question])\n",
        "    return response.text  # Return LLM response\n",
        "\n",
        "def analyze_images_with_vlm(images):\n",
        "    \"\"\"Analyze images using Gemini Vision (VLM).\"\"\"\n",
        "    if not images:\n",
        "        return \"No images found in the PDF.\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
        "\n",
        "    image_parts = []\n",
        "    for img in images:\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        img.save(img_byte_arr, format=img.format)\n",
        "        img_byte_arr = img_byte_arr.getvalue()\n",
        "\n",
        "        image_parts.append({\n",
        "            \"mime_type\": \"image/\" + img.format.lower(),\n",
        "            \"data\": img_byte_arr\n",
        "        })\n",
        "\n",
        "\n",
        "    response = model.generate_content([\"Analyze these images and summarize their content:\"] + image_parts)\n",
        "\n",
        "    return response.text\n",
        "\n",
        "\n",
        "pdf_path = \"/content/DeepSeek_R1.pdf\"\n",
        "\n",
        "pdf_text = extract_pdf_text(pdf_path)\n",
        "pdf_images = extract_pdf_images(pdf_path)\n",
        "\n",
        "image_analysis = analyze_images_with_vlm(pdf_images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "xGQPoYP5rCdo",
        "outputId": "b15729e3-b701-4b8e-adf5-2bf2325d878b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask a question about the PDF (or type 'exit' to quit): tell me about the images\n",
            "\n",
            "ðŸ”¹ Text-Based Answer: The image you provided appears to be a screenshot of a research paper titled \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\" by DeepSeek-AI.  The screenshot includes the abstract, table of contents, introduction, and a figure showing benchmark performance.\n",
            "\n",
            "Key takeaways from the visible sections:\n",
            "\n",
            "* **Focus on Reasoning:** The paper introduces DeepSeek-R1, a language model designed to improve reasoning capabilities.  They've developed two versions: DeepSeek-R1-Zero, trained purely through reinforcement learning (RL), and DeepSeek-R1, which incorporates cold-start data and a multi-stage training process.\n",
            "* **Reinforcement Learning Emphasis:**  A major contribution is the use of large-scale RL to improve reasoning, even without initial supervised fine-tuning.  DeepSeek-R1-Zero demonstrates this capability, though it suffers from readability and language mixing issues. DeepSeek-R1 addresses these issues.\n",
            "* **Open-Sourcing:**  The team has open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and several distilled smaller models based on Qwen and Llama, making their research accessible to the community.\n",
            "* **Performance:**  The benchmark figure compares DeepSeek-R1 and related models (including OpenAI models) on various reasoning tasks like AIME, Codeforces, GPQA, MATH, MMLU, and SWE-bench. DeepSeek-R1 achieves comparable or better performance than some OpenAI models on certain benchmarks.\n",
            "* **Distillation:**  The paper also highlights the success of distilling the reasoning abilities of DeepSeek-R1 into smaller, more efficient models.\n",
            "\n",
            "The table of contents indicates further sections detailing the approach, experiments, discussion, and conclusion, which would contain more specific information about the training process, data used, and analysis of the results.\n",
            "\n",
            "\n",
            "ðŸ”¹ Image Analysis (VLM): Here's a summary of the content in the images:\n",
            "\n",
            "**Image 1: Accuracy During Training**\n",
            "\n",
            "*   **Title:** \"DeepSeek-R1-Zero AIME accuracy during training\"\n",
            "*   **Content:** The image shows the accuracy of two different models (\"r1-zero-pass@1\" and \"r1-zero-cons@16\") during training. The x-axis represents the training steps, and the y-axis represents the accuracy.\n",
            "*   **Key Observations:**\n",
            "    *   The \"r1-zero-cons@16\" model (red line) consistently achieves higher accuracy than the \"r1-zero-pass@1\" model (blue line) throughout the training process.\n",
            "    *   Both models show an increasing trend in accuracy as the training progresses.\n",
            "    *   Two horizontal lines: A green dashed line indicating \"o1-0912-pass@1\" accuracy (around 0.74) and a purple dashed line indicating \"o1-0912-cons@64\" accuracy (around 0.83). These likely represent benchmark performances.\n",
            "\n",
            "**Image 2: Average Length per Response During Training**\n",
            "\n",
            "*   **Title:** \"DeepSeek-R1-Zero average length per response during training\"\n",
            "*   **Content:**  The image illustrates the average length of the response (likely in tokens or characters) generated by a model as training progresses. The x-axis shows training steps, and the y-axis represents the average length per response.\n",
            "*   **Key Observations:**\n",
            "    *   The average response length tends to increase as the training steps increase.  The line shows an overall upward trajectory.\n",
            "    *   There is a significant amount of variability (indicated by the shaded area) in the average response length at any given training step, implying that the model's output length fluctuates.\n",
            "\n",
            "Ask a question about the PDF (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_question = input(\"\\nAsk a question about the PDF (or type 'exit' to quit): \")\n",
        "    if user_question.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    text_answer = ask_gemini_llm(pdf_text, user_question)\n",
        "\n",
        "    print(\"\\nðŸ”¹ Text-Based Answer:\", text_answer)\n",
        "    print(\"\\nðŸ”¹ Image Analysis (VLM):\", image_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgCIEorvqsiE",
        "outputId": "6bc1169f-81a1-4efd-e567-2e3088099456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¹ Extracted Text from PDF:\n",
            "\n",
            "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
            "Reinforcement Learning\n",
            "DeepSeek-AI\n",
            "research@deepseek.com\n",
            "Abstract\n",
            "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
            "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
            "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
            "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
            "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
            "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
            "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
            "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
            "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
            "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based o\n"
          ]
        }
      ],
      "source": [
        "pdf_text = extract_pdf_text(pdf_path)\n",
        "\n",
        "# ðŸ”¹ Print the extracted text\n",
        "print(\"\\nðŸ”¹ Extracted Text from PDF:\\n\")\n",
        "print(pdf_text[:1000])  # Print the first 1000 characters (to avoid too much output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "213baMILq0un",
        "outputId": "5eb7cefc-d57c-4e99-8c8b-a9baa7cb5ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¹ Extracted 2 images from the PDF.\n"
          ]
        }
      ],
      "source": [
        "pdf_images = extract_pdf_images(pdf_path)\n",
        "\n",
        "# ðŸ”¹ Show extracted images\n",
        "print(f\"\\nðŸ”¹ Extracted {len(pdf_images)} images from the PDF.\")\n",
        "\n",
        "# Display the first few images\n",
        "for i, img in enumerate(pdf_images[:5]):  # Show only first 5 images\n",
        "    img.show(title=f\"Extracted Image {i+1}\")  # Opens image in the default viewer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxr6VRD1q4KO",
        "outputId": "7c73479b-9a10-443b-ea5e-f672228acfe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved extracted_image_1.png\n",
            "Saved extracted_image_2.png\n"
          ]
        }
      ],
      "source": [
        "for i, img in enumerate(pdf_images):\n",
        "    img.save(f\"extracted_image_{i+1}.png\")  # Save as PNG\n",
        "    print(f\"Saved extracted_image_{i+1}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLQx0l8NrtqE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3raW9QZ2sSad"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
